{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic RL Implementation Code to find rewards accumulated on moving through 20 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a dummy environment to give the agent some random rewards regardless of the agent's actions\n",
    "\n",
    "environment class must be capable of handling actions received from the agent. This is done by checking the number of steps left and returning a random reward, ignoring the agent's actions\n",
    "\n",
    "__init__ constructor is called to set the number of episoded for the event, get_observation() method to return the current environment's observation to the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "class SampleEnvironment:    # class for environment   \n",
    "    def __init__(self): # constructor called init\n",
    "        self.steps_left = 20     # defines a variable called steps_left\n",
    "\n",
    "    def get_observation(self) -> List[float]:\n",
    "        return [0.0, 0.0, 0.0]  # co-ordinates for environment\n",
    "    \n",
    "    def get_actions(self) -> List[int]: # used to give reward 0 = negative and 1 = reward\n",
    "        return [0,1]\n",
    "    \n",
    "    def is_done(self) -> bool:  # bool value to check if steps have been completed and to stop\n",
    "        return self.steps_left == 0\n",
    "    \n",
    "    def action(self, action: int) -> float:   # will check if there are steps are left\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\")\n",
    "        self.steps_left -=1\n",
    "        return random.random()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent's class includes two methods: the constructor and the method that performs one step in the environment.\n",
    "\n",
    "Initially, the total reward collected is set to zero by the constructor.\n",
    "\n",
    "The step function accepts the environment instance as an argument and allows the agent to perform the following actions:<br  />\n",
    "\n",
    "1. Observe the environment <br  />\n",
    "2. Make a decision about the action to take based on the observations<br  />\n",
    "3. Submit the action to the environment<br  />\n",
    "4. Get the reward for the current step<br  />\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:    # class to define the agent\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0 # reward is defined as 0\n",
    "\n",
    "    # agent obeserves the envt and makes a decision to get the reward\n",
    "    \n",
    "    def step(self, env: SampleEnvironment): # step takes sample environment object\n",
    "        current_obs = env.get_observation() # hardcoded as 0,0,0\n",
    "        print(\"Observation {} \".format(current_obs))\n",
    "\n",
    "        actions = env.get_actions() # we can pick 0 or 1 randomly\n",
    "        print(actions)\n",
    "\n",
    "        reward = env.action(random.choice(actions)) # to implement action by randomly picking a reward\n",
    "        self.total_reward +=reward\n",
    "        print(\"Total Reward {}\".format(self.total_reward))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 0.8339019818513054\n",
      "Step 2\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 1.7324122332119103\n",
      "Step 3\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 1.9735116903405503\n",
      "Step 4\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 2.1134843051107204\n",
      "Step 5\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 2.1256447456066976\n",
      "Step 6\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 3.0670933131445546\n",
      "Step 7\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 3.5682842522062934\n",
      "Step 8\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 3.821802132378659\n",
      "Step 9\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 4.66321294289627\n",
      "Step 10\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 5.607707116077869\n",
      "Step 11\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 6.076759302918814\n",
      "Step 12\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 6.63555145017288\n",
      "Step 13\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 7.414232106795048\n",
      "Step 14\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 7.471699267141289\n",
      "Step 15\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 7.8852728774724214\n",
      "Step 16\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 7.936707404632436\n",
      "Step 17\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 8.678854537504966\n",
      "Step 18\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 9.262522232872916\n",
      "Step 19\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 9.724765163399251\n",
      "Step 20\n",
      "Observation [0.0, 0.0, 0.0] \n",
      "[0, 1]\n",
      "Total Reward 10.61472860522343\n",
      "Total reward got 10.6147\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = SampleEnvironment()\n",
    "    agent = Agent()\n",
    "    i=0\n",
    "\n",
    "    while not env.is_done():\n",
    "        i=i+1\n",
    "        print(\"Step {}\".format(i))\n",
    "        agent.step(env)\n",
    "\n",
    "    print(\"Total reward got %.4f\" % agent.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
